{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment_Analysis_TFIDF_word2vec_LSTM_GRU.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1c5vXHuFF7gFuM_BhsiJe7JtfslBk9Bi4","authorship_tag":"ABX9TyMgVsyTVEYMSHLI94Hx48kJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ma12esm2hBVO","colab_type":"text"},"source":["## Sentiment Analysis using TFIDF and LSTM"]},{"cell_type":"code","metadata":{"id":"6bxiDamcpv-f","colab_type":"code","outputId":"61fcbf1a-d30f-4926-9991-ba614c5ccaaa","executionInfo":{"status":"ok","timestamp":1591201295764,"user_tz":-330,"elapsed":2324,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import numpy as np\n","tf.__version__"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.2.0'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"2xXnazUDjn0k","colab_type":"code","colab":{}},"source":["## Import dataset\n","dataset = pd.read_csv(\"https://raw.githubusercontent.com/atulpatelDS/Data_Files/master/Bag_of_Words/word2vec_nlp/labeledTrainData.tsv.zip\",\n","                      header=0,delimiter=\"\\t\",quoting=3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uua0wZWdjn3d","colab_type":"code","outputId":"dd32a583-9c38-435f-cc5f-3d9b60760f87","executionInfo":{"status":"ok","timestamp":1591201311926,"user_tz":-330,"elapsed":746,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":206}},"source":["dataset.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sentiment</th>\n","      <th>review</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\"5814_8\"</td>\n","      <td>1</td>\n","      <td>\"With all this stuff going down at the moment ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>\"2381_9\"</td>\n","      <td>1</td>\n","      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"7759_3\"</td>\n","      <td>0</td>\n","      <td>\"The film starts with a manager (Nicholas Bell...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\"3630_4\"</td>\n","      <td>0</td>\n","      <td>\"It must be assumed that those who praised thi...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"9495_8\"</td>\n","      <td>1</td>\n","      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id  sentiment                                             review\n","0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n","1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n","2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n","3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n","4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"9F5qbtyXjn6K","colab_type":"code","colab":{}},"source":[" reviews = dataset[\"review\"].tolist()\n"," sentiment = np.array(dataset[\"sentiment\"].tolist())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CR6PcgLolQ8H","colab_type":"code","outputId":"429e0874-95cb-4b7a-c38b-d1a909687e1c","executionInfo":{"status":"ok","timestamp":1591201325817,"user_tz":-330,"elapsed":1485,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["len(reviews),len(sentiment)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 25000)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"DFn2JHxrlQ-f","colab_type":"code","outputId":"db847246-5f9b-48bb-964d-ac1caae01870","executionInfo":{"status":"ok","timestamp":1591201330842,"user_tz":-330,"elapsed":1040,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":56}},"source":["reviews[0]"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\\'s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\\'s music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\\'s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\\'ve gave this subject....hmmm well i don\\'t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"ztEjvILLlRAv","colab_type":"code","colab":{}},"source":["## Convert text to number using kears\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(reviews)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rf-wt7hhmoxm","colab_type":"code","outputId":"2df3aab6-8fb1-40db-a534-317686934a61","executionInfo":{"status":"ok","timestamp":1591201364945,"user_tz":-330,"elapsed":1266,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["len(tokenizer.word_index)  ## these are total words but we are going to select only top 5000 unique word for TFIDF"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["88582"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"gHtvcLHdmo0d","colab_type":"code","colab":{}},"source":["## convert text to TFIDF\n","input_data = tokenizer.texts_to_matrix(reviews,mode=\"tfidf\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zS2p-rUpOGE","colab_type":"code","outputId":"1bb4e654-2463-4bf9-d7c9-48a203cd0236","executionInfo":{"status":"ok","timestamp":1591201408539,"user_tz":-330,"elapsed":2362,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["input_data.shape"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 5000)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"kPRcF81ZpOI2","colab_type":"code","outputId":"442448c6-2e69-44cb-ad0a-cd2708bea9e7","executionInfo":{"status":"ok","timestamp":1591201413644,"user_tz":-330,"elapsed":1186,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["input_data[0]"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.        , 2.75042893, 2.34574918, ..., 0.        , 0.        ,\n","       0.        ])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"MZdmHAMBpO7V","colab_type":"code","colab":{}},"source":["## Build the LSTM Model\n","## Initilize the model\n","model = keras.models.Sequential()\n","## Reshape the data from 1D to 2D becuase LSTM takes 2D input\n","model.add(keras.layers.Reshape((5000,1),input_shape=(5000,))) ## input shape--we need to feed input review row via row\n","## Normalize the input data\n","model.add(keras.layers.BatchNormalization())\n","## add LSTM Layer\n","model.add(keras.layers.LSTM(128)) ## size of the memory=128 for both cell and hidden state\n","## what is time stamp this need to remember ,it need to lookup 6K words and each word is represengted by single number.so ^K is the time stamp.\n","## We are only interested to get final output. and we need output 1 and if it is between 0 and 1 we will use sigmoid."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jgIPloMnpO-H","colab_type":"code","colab":{}},"source":["## Output layer\n","model.add(keras.layers.Dense(1,activation=\"sigmoid\"))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p8G7pGfMmo2i","colab_type":"code","colab":{}},"source":["## compile the model\n","model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wlCGW4Omo5n","colab_type":"code","outputId":"6b65e1bb-2373-4fc0-fcf9-414c642ec0e0","executionInfo":{"status":"ok","timestamp":1591201849675,"user_tz":-330,"elapsed":279836,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"source":["## Train the model\n","model.fit(x=input_data,y=sentiment,validation_split=0.2,epochs=5,batch_size=128)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","157/157 [==============================] - 54s 345ms/step - loss: 0.6927 - accuracy: 0.4990 - val_loss: 0.6915 - val_accuracy: 0.4954\n","Epoch 2/5\n","157/157 [==============================] - 54s 341ms/step - loss: 0.6921 - accuracy: 0.4986 - val_loss: 0.6912 - val_accuracy: 0.5102\n","Epoch 3/5\n","157/157 [==============================] - 54s 341ms/step - loss: 0.6920 - accuracy: 0.5045 - val_loss: 0.6913 - val_accuracy: 0.4984\n","Epoch 4/5\n","157/157 [==============================] - 54s 342ms/step - loss: 0.6920 - accuracy: 0.4964 - val_loss: 0.6912 - val_accuracy: 0.5108\n","Epoch 5/5\n","157/157 [==============================] - 54s 342ms/step - loss: 0.6918 - accuracy: 0.5028 - val_loss: 0.6915 - val_accuracy: 0.4968\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f93290a28d0>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"3uSUE4oewtdb","colab_type":"text"},"source":["It feeded all the 5000 words from each review one by one and at last of each review word it gives the output(ht).RNN can work with sequence data, but TFIDF doesn't provide the output in sequence although It provide the output as per the index of the words, which do not followup how they appear in a perticular document(sentence).SO we can notice that it is very very slow to train the data."]},{"cell_type":"markdown","metadata":{"id":"vEVRQY7O434_","colab_type":"text"},"source":["## Sentiment Analysis using Pretrained word2vec and LSTM"]},{"cell_type":"code","metadata":{"id":"vD4wSeFshHH9","colab_type":"code","outputId":"d38f98d1-7eae-4d3a-d5fd-21d233fe74b5","executionInfo":{"status":"ok","timestamp":1591201870433,"user_tz":-330,"elapsed":1037,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":206}},"source":["dataset.head()"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sentiment</th>\n","      <th>review</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\"5814_8\"</td>\n","      <td>1</td>\n","      <td>\"With all this stuff going down at the moment ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>\"2381_9\"</td>\n","      <td>1</td>\n","      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"7759_3\"</td>\n","      <td>0</td>\n","      <td>\"The film starts with a manager (Nicholas Bell...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\"3630_4\"</td>\n","      <td>0</td>\n","      <td>\"It must be assumed that those who praised thi...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"9495_8\"</td>\n","      <td>1</td>\n","      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id  sentiment                                             review\n","0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n","1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n","2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n","3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n","4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"oZBP8op-5ec7","colab_type":"code","colab":{}},"source":["## we are going to use same above text to number Tokenizer\n","## Instead of convert text to TFIDF we will convert Text to sequence. this convert each review's word to index number\n","reviews_seq = tokenizer.texts_to_sequences(reviews)\n","## Where are keeping track of word sequence in each review "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4YgogQ3K5efn","colab_type":"code","colab":{}},"source":["#reviews[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XXuvZVo45eiA","colab_type":"code","colab":{}},"source":["#reviews_seq[0] "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1aSAW5U5ekW","colab_type":"code","colab":{}},"source":["#tokenizer.word_index"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I5uVnVDu5emI","colab_type":"code","outputId":"e0f9e4a7-3d31-44d9-aa5d-901f1d5b8e70","executionInfo":{"status":"ok","timestamp":1591201902952,"user_tz":-330,"elapsed":1185,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["## Lets check the lenght of each review\n","len(reviews_seq[0]),len(reviews_seq[1])"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(403, 148)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"RpV7DuDQNNcl","colab_type":"text"},"source":["as we see that lenght of each reviews is not same and we need make all these same lenght, so we will use padding"]},{"cell_type":"code","metadata":{"id":"-HJHWFntLJxb","colab_type":"code","colab":{}},"source":["reviews_seq = keras.preprocessing.sequence.pad_sequences(reviews_seq,\n","                                                         maxlen= 300## max review lenght can change as per your experience and dataset\n","                                                         ,padding=\"pre\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gogZkr-JLJ0G","colab_type":"code","outputId":"01742147-2a8d-4911-963d-39793c58b56e","executionInfo":{"status":"ok","timestamp":1591201911235,"user_tz":-330,"elapsed":1237,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["len(reviews_seq[0]),len(reviews_seq[1])"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(300, 300)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"-FOMMR3XLJ2b","colab_type":"code","outputId":"ebe8b513-e4ca-4eb3-dbc6-e133a86a3a38","executionInfo":{"status":"ok","timestamp":1591201913579,"user_tz":-330,"elapsed":1002,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":541}},"source":["reviews_seq[1]"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    1,  353,\n","        322,    4,    1, 3249,   31, 3734,    6,    3,   52,  438,   19,\n","         12,  537,  268,    5,   84,  778,    2,    5, 2016, 1328,  353,\n","        271,  440, 2876,    8,  396,   35,   10,    2,  145,   34,  293,\n","         24,   19,   16,   69, 2521,    1,  189,   12,    9,   13,   21,\n","          1, 1270,  724,  359, 2387,   12,  263,   43,  172,  288,  959,\n","       1328,    1, 3657,  307,   16,  824, 3765,   12,   66,   61,    1,\n","       3626, 4043,    5,    1,  271,  537,  313,  269,   15,  272,  180,\n","          8,    3,   17,  145,   34,  529,   14, 2365, 1414,  165,   61,\n","          5,  282,   33,   67,  405,  964,    3,   17,   20,   50,  671,\n","         37,  109, 2161,   60,    6,  135,   88,   81,  112, 1037,   16,\n","          1, 1414,   72,  506,    1,  778,  440,  273,   80,  109, 2719,\n","          5, 2016, 1328,  353,  664,    2,   72,  255,    9,    5,   27,\n","         52,  438,   11,   90,    9,  773,    5, 4892,   48,    1, 1414,\n","          5,   27,   91], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"iKwaGBSeLJ4M","colab_type":"code","outputId":"276b5bf9-63d1-465f-8ebd-15cedff991f4","executionInfo":{"status":"ok","timestamp":1591201917887,"user_tz":-330,"elapsed":1273,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":541}},"source":["reviews_seq[0]"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2006, 1156,   18,    4,  261,   11,    6,   29,   41,  485, 1878,\n","         35,  891,   22, 2588,   37,    8,  550,   92,   22,   23,  167,\n","          5,  780,   11,    2,  166,    9,  354,   46,  200,  680,   32,\n","         15,    5,    1,  228,    4,   11,   17,   18,    2,   88,    4,\n","         24,  448,   59,  132,   12,   26,   90,    9,   15,    1,  448,\n","         60,   45,  280,    6,   63,  324,    4,   87,    7,    7,    1,\n","        776,  788,   19,  224,   51,    9,  414,  514,    6,   61,   20,\n","         15,  888,  231,   39,   35,    1, 3537, 1670,  717,    2,  911,\n","          6, 1075,   14,    3,   29,  972, 1389, 1631,  135,   26,  490,\n","        348,   35,   75,    6,  721,   69,   85,   24, 2454,  911,  106,\n","         12,   26,  470,   81,    5,  121,    9,    6,   26,   34,    6,\n","       1664,  520,   35,   10,  276,   26,   40, 4138,  225,    7,    7,\n","        772,    4,  643,  180,    8,   11,   37, 1583,   80,    3,  516,\n","          2,    3, 2353,    2,    1,  223, 2119, 2718,  717,   79,    1,\n","        164,  212,   25,   66,    1,    4,    3,   51,    9,  382,    5,\n","       1418,    1,   75,  717,   14,  628,  904,  780,  777,   16,   28,\n","        551,  384,  581,    3,  223,  758,    4,   95, 3433,    3, 1311,\n","        833,  133,    7,    7, 1321,  344,   11,   17,    6,   15,   81,\n","         34,   37,   20,   28,  646,   39,  157,   60,   10,  101,    6,\n","         88,   81,   45,   21,   92,  785,  242,    9,  124,  350,    2,\n","        199,  122,    3,  746,    2, 3606, 2064,    8,   11,   17,    6,\n","          3,  247,  485, 1878,    6,  368,   28,    4,    1,   88, 1016,\n","         81,  123,    5, 1693,   11, 1220,   18,    6,   26, 2512,   70,\n","         16,   29,    1,  688,  204,  517,   11,  872,   70,   10,   89,\n","        121,   85,   81,   67,   27,  272,  493, 4558, 3584,   10,  121,\n","         11,   15,    3,  189,   26,    6,  342,   32,  573,  324,   18,\n","        375,  229,   39,   28,    4,    1,   88,   10,  437,   26,    6,\n","         21,    1, 1559], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"Uq04xW3sLJ6e","colab_type":"code","colab":{}},"source":["## lets use pretrained word2vec model\n","## use path to get details how we train word2vec\n","## https://github.com/atulpatelDS/NLP/blob/master/Word2vec.ipynb\n","from gensim.models import Word2Vec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qKpyei-9Q5t4","colab_type":"code","outputId":"d340e1ab-9e33-4a48-fe9d-2dfc32dab5d7","executionInfo":{"status":"ok","timestamp":1591201994431,"user_tz":-330,"elapsed":1268,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["word2vec_model=Word2Vec.load(\"sample_data/word2vec-movie-IMDB\")"],"execution_count":27,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"GuoEJeOVQ5w8","colab_type":"code","outputId":"83bf1702-b01e-49b6-e387-35235875f6d3","executionInfo":{"status":"ok","timestamp":1591202014041,"user_tz":-330,"elapsed":977,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["## Lets check the shape of loaded model\n","word2vec_model.wv.vectors.shape"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(28322, 50)"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"ooyuNcAzRuPI","colab_type":"text"},"source":["in TFIDF we have 1 number for each word and in word2vec we have 50 number for each word.\n","We need to create the array of 5000 words so I need word2vec embedding."]},{"cell_type":"code","metadata":{"id":"9PUGo7yATE-1","colab_type":"code","outputId":"5e60396f-1e63-4ac1-efb3-7b0c36740b63","executionInfo":{"status":"ok","timestamp":1591202032915,"user_tz":-330,"elapsed":1153,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["embedding_vector_length = word2vec_model.wv.vectors.shape[1]\n","embedding_vector_length"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"U0CS5OIjSgjS","colab_type":"code","outputId":"5cdebfdc-6aed-435d-c476-f290a3cf6ca8","executionInfo":{"status":"ok","timestamp":1591202056214,"user_tz":-330,"elapsed":1132,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["max_words = 5000\n","## 1 becuase we also need to add pad sequence\n","embedding_matrix = np.zeros((max_words+1,embedding_vector_length))\n","embedding_matrix.shape ## matrix with all zero value"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5001, 50)"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"Ut0mkKUFSgoW","colab_type":"code","colab":{}},"source":["#tokenizer.word_index.items()\n","#word2vec_model.wv.vocab\n","#word2vec_model.wv[\"hard\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bkUlFvqQSgls","colab_type":"code","colab":{}},"source":["## lets fillup these zero values with actual vectors \n","for word,i in sorted(tokenizer.word_index.items(),key=lambda x:x[1]):\n","  if i > max_words:  ## ignore all words greated than 5000\n","    break\n","  if word in word2vec_model.wv.vocab:\n","    embedding_vector=word2vec_model.wv[word]\n","    embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R3Twt9P_YVP4","colab_type":"code","outputId":"91d7e0fb-2e86-477d-f86e-27542ae1e558","executionInfo":{"status":"ok","timestamp":1591202064588,"user_tz":-330,"elapsed":999,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["#embedding_matrix[1]\n","embedding_matrix[0] ## get zero becuase there is no word2vec embedding in the pretrained model for blank spaces"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"DLMevGfVYVjY","colab_type":"code","colab":{}},"source":["## Now we have word2vec numbers for 5000 words\n","## Lets buld the graph\n","model_wv = keras.models.Sequential()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kf0I8eqcd7HP","colab_type":"text"},"source":["The Embedding layer is used to create word vectors for incoming words. It sits between the input and the LSTM layer, i.e. the output of the Embedding layer is the input to the LSTM layer.\n","\n","The weights for the Embedding layer can either be initialized with random values, or more commonly, they are initialized with third-party word embeddings such as word2vec, GloVe or fasttext (or others) and these weights can optionally be fine-tuned during training.Using third party embeddings to build word vectors is as a form of transfer learning, since you transfer the semantic information between words that was learned during the embedding process."]},{"cell_type":"code","metadata":{"id":"2G-L2980ZdO3","colab_type":"code","colab":{}},"source":["## Add embedding Layer\n","max_review_length = 300\n","## input for embedding layer (batch_size,max review lenght)  ## max review length = 300\n","## output from embedding layer = (batch_size,300,50)  ## 50>ebedding size in word2vec\n","model_wv.add(keras.layers.Embedding(5001,embedding_vector_length,input_length=max_review_length,\n","                                 weights=[embedding_matrix],trainable = False))\n","## If we dont have pre traineed embedding then we can set trainable = True t\n","## and remove the weights=[embedding_matrix]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-CLCdzqZdRS","colab_type":"code","outputId":"6c16add0-0bfe-4d93-8a24-974660e68194","executionInfo":{"status":"ok","timestamp":1591202087002,"user_tz":-330,"elapsed":1031,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":56}},"source":["## Add LSTM LAyer\n","model_wv.add(keras.layers.LSTM(128,dropout=0.2,recurrent_dropout=0.2))\n","## recurrent_dropout means it apply before the lstm layer\n"],"execution_count":36,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OuS5VJnsgdXl","colab_type":"code","colab":{}},"source":["## Add ouput layer\n","model_wv.add(keras.layers.Dense(1,activation=\"sigmoid\"))\n","## compile the model\n","model_wv.compile(optimizer=\"adam\",metrics=\"accuracy\",loss=\"binary_crossentropy\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zXsk2Veygdcb","colab_type":"code","outputId":"d51daa43-beb3-4e2f-cc3e-6f1792361c17","executionInfo":{"status":"ok","timestamp":1591202874236,"user_tz":-330,"elapsed":771102,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"source":["## Tarin the model\n","model_wv.fit(reviews_seq,sentiment,validation_split=0.2,\n","          epochs=5,\n","          batch_size=128)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","157/157 [==============================] - 155s 987ms/step - loss: 0.5197 - accuracy: 0.7395 - val_loss: 0.4623 - val_accuracy: 0.7868\n","Epoch 2/5\n","157/157 [==============================] - 152s 968ms/step - loss: 0.4212 - accuracy: 0.8140 - val_loss: 0.3964 - val_accuracy: 0.8320\n","Epoch 3/5\n","157/157 [==============================] - 152s 969ms/step - loss: 0.3798 - accuracy: 0.8359 - val_loss: 0.3570 - val_accuracy: 0.8454\n","Epoch 4/5\n","157/157 [==============================] - 153s 974ms/step - loss: 0.3460 - accuracy: 0.8523 - val_loss: 0.3254 - val_accuracy: 0.8568\n","Epoch 5/5\n","157/157 [==============================] - 151s 963ms/step - loss: 0.3272 - accuracy: 0.8616 - val_loss: 0.3241 - val_accuracy: 0.8674\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f919c8967f0>"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"CJuI2GOHCeXy","colab_type":"text"},"source":["1. Memory Size = 128 : Same in TFIDF and word2vec\n","2. Time steps = TFIDF: 5000, W2Vec: 300 -> Better for w2vec : 17 times. This can we change as per your requirement\n","3. Input size at each time step: TFIDF: 1 W2vec: 50 -> Better for TFIDF\n","4. Sequencing of words in a Document: TFIDF has no clue of sequencing although word2vec has sequencing.\n","5. Accuracy: TFIDF very"]},{"cell_type":"code","metadata":{"id":"OWvtvjhSGJl2","colab_type":"code","colab":{}},"source":["## Lets use GRU layer instead of LSTM\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vvd68NSwGJom","colab_type":"code","colab":{}},"source":["## Now we have word2vec numbers for 5000 words\n","## Lets buld the graph\n","model_gru = keras.models.Sequential()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y9k4Dp0hGJq7","colab_type":"code","colab":{}},"source":["## Add embedding Layer\n","max_review_length = 300\n","## input for embedding layer (batch_size,max review lenght)  ## max review length = 300\n","## output from embedding layer = (batch_size,300,50)  ## 50>ebedding size in word2vec\n","model_gru.add(keras.layers.Embedding(5001,embedding_vector_length,input_length=max_review_length,\n","                                 weights=[embedding_matrix],trainable = False))\n","## If we dont have pre traineed embedding then we can set trainable = True t\n","## and remove the weights=[embedding_matrix]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nAiPKTxdGJtS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":56},"outputId":"0dbf331e-7e62-4c7a-b33b-177f53a200f9","executionInfo":{"status":"ok","timestamp":1591203217448,"user_tz":-330,"elapsed":1090,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}}},"source":["## Add LSTM LAyer\n","model_gru.add(keras.layers.GRU(128,dropout=0.2,recurrent_dropout=0.2))\n","## recurrent_dropout means it apply before the lstm layer\n"],"execution_count":41,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WjMrp5t-GJvP","colab_type":"code","colab":{}},"source":["## Add ouput layer\n","model_gru.add(keras.layers.Dense(1,activation=\"sigmoid\"))\n","## compile the model\n","model_gru.compile(optimizer=\"adam\",metrics=\"accuracy\",loss=\"binary_crossentropy\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LVAjmDfWGmh6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":243},"outputId":"41172e4e-2721-4814-d85f-356a2b77d35c","executionInfo":{"status":"ok","timestamp":1591203946870,"user_tz":-330,"elapsed":691160,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}}},"source":["## Tarin the model\n","model_gru.fit(reviews_seq,sentiment,validation_split=0.2,\n","          epochs=5,\n","          batch_size=128)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","157/157 [==============================] - 139s 883ms/step - loss: 0.5666 - accuracy: 0.6977 - val_loss: 0.5046 - val_accuracy: 0.7666\n","Epoch 2/5\n","157/157 [==============================] - 137s 875ms/step - loss: 0.4397 - accuracy: 0.8038 - val_loss: 0.3934 - val_accuracy: 0.8382\n","Epoch 3/5\n","157/157 [==============================] - 137s 871ms/step - loss: 0.3718 - accuracy: 0.8389 - val_loss: 0.3307 - val_accuracy: 0.8604\n","Epoch 4/5\n","157/157 [==============================] - 136s 864ms/step - loss: 0.3316 - accuracy: 0.8594 - val_loss: 0.3209 - val_accuracy: 0.8670\n","Epoch 5/5\n","157/157 [==============================] - 136s 863ms/step - loss: 0.3074 - accuracy: 0.8696 - val_loss: 0.2959 - val_accuracy: 0.8742\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f919b24eba8>"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"feG64-MNGmks","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}